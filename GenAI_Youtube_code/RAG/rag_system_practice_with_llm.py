# -*- coding: utf-8 -*-
"""


author : @akash


RAG_System_Practice_with_LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19ZJvHqRNb_TEe1pOCRCkxeQrVn8UmT13




RAG system 5 steps:

1). data loading
2). split the data into chunks
3). create embedding for these chunks
4). store embedding in vector database
5). Retrieval the query related chunks from vector database

#install required pakages


!pip install langchain_community
!pip install openai
!pip install chromadb
!pip install tiktoken
!pip install cohere


"""


#llm api keys

import os
from google.colab import userdata
#userdata.get('secretName')

os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')
os.environ["COHERE_API_KEY"] = userdata.get('COHERE_API_KEY')

#step 1
#now load the your private data or document loading from any sources

#!pip install pypdf --quiet

#!wget https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Agreements/EMPLOYEE_AGREEMENT.pdf

from langchain.document_loaders import PyPDFLoader
loader=PyPDFLoader("EMPLOYEE_AGREEMENT.pdf")
pages=loader.load()
print(len(pages))

#steps2
#split data into chunks

from langchain.text_splitter import RecursiveCharacterTextSplitter
#choose chuks size as 10% generally because of llm have limited no. of tokens at a time to generate the output
text_splitter=RecursiveCharacterTextSplitter(chunk_size=300,chunk_overlap=30)
chunks=text_splitter.split_documents(pages)
print(len(chunks))

#steps 3
#convert chunks to embedding


from langchain.embeddings import OpenAIEmbeddings, CohereEmbeddings

embeddings=CohereEmbeddings(user_agent="langchain")
#embeddings=OpenAIEmbeddings()


"""
!pip install chromadb


"""


#steps 4
#store the embedding in vector database -->chromadb, faiss, eleastic-search

from langchain.vectorstores import Chroma
emp_rules_db= Chroma.from_documents(chunks,
                                    embeddings,
                                    persist_directory="emp_rules_db"
                          )
emp_rules_db.persist()

#step 5
#retrieval the chunks from vector database based on query embedding match


retriever = emp_rules_db.as_retriever()
result=retriever.get_relevant_documents("What is the policy for sick leaves",
                                        top_k=3)
result

#4 document has retrieved based in query similarity

from langchain.retrievers import MultiQueryRetriever
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI, Cohere

llm=Cohere(temperature=0)

llm_based_retriever=MultiQueryRetriever.from_llm(llm=llm,
                                                 retriever = emp_rules_db.as_retriever(),
                                                )

#your own custom GPT works only on your private data
llm_based_retriever  #--->this is your custom GPT

from langchain.chains import RetrievalQA
#llm=OpenAI(temperature=0)
llm=Cohere(temperature=0)

Q_AChain=RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",# It takes a list of documents, inserts them all into a prompt
    retriever=llm_based_retriever
)

query="who is Akash?"
docs=Q_AChain({"query":query})
docs["result"]

