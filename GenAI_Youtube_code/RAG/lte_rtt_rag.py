# -*- coding: utf-8 -*-
"""


LTE_RTT_RAG.ipynb
Automatically generated by Colab.
Original file is located at
https://colab.research.google.com/drive/16RA7OcDBtCUGdlx1C7_7Cp0p5sDa-W3Q
    
    
    
author : @akash
"""


"""
!pip install openai --quiet
!pip install langchain --quiet
!pip install cohere --quiet
!pip install tiktoken --quiet
!pip install langchain_community
!pip install chromadb -q
"""



import os
#os.environ['OPENAI_API_KEY'] = "Your own OPENAI_API_KEY"
os.environ['COHERE_API_KEY'] = "cHpZ6GpjYJJEmrlNq2YgTPvrLKaBmwxU1fvxTp7x"

#Better way
#from google.colab import userdata
#os.environ['OPENAI_API_KEY'] = userdata.get("OPENAI_API_KEY")
#os.environ['COHERE_API_KEY'] = userdata.get("COHERE_API_KEY")

def read_rtt_file(filepath):
    data = []
    try:
        with open(filepath, 'r') as f:
            for line in f:
                data.append(line.strip())  # Remove leading/trailing whitespace
    except FileNotFoundError:
        print(f"Error: File not found at {filepath}")
        return None
    return data

filepath = r"C:\Akash\GenAI_YouTube\GenAI_Youtube_code.rtt"
my_data = read_rtt_file(filepath)
if my_data:
    print(my_data) # Access the data

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)

# Assuming my_data is a list of strings
documents = [Document(page_content=text) for text in my_data]

chunks = text_splitter.split_documents(documents)
print(len(chunks))

#!pip install chromadb -q

from langchain.embeddings import OpenAIEmbeddings, CohereEmbeddings

#embeddings = OpenAIEmbeddings()
embeddings = CohereEmbeddings(user_agent="langchain")

from langchain.vectorstores import Chroma

emp_rules_db= Chroma.from_documents(chunks,
                                    embeddings,
                                    persist_directory="emp_rules_db"
                          )
emp_rules_db.persist()

retriever = emp_rules_db.as_retriever()
result=retriever.get_relevant_documents("what is MCC and MNC?",
                                        top_k=3)
result

# Helper function for printing docs


def pretty_print_docs(docs):
    print(
        f"\n{'-' * 100}\n".join(
            [f"Document {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]
        )
    )

retriever = emp_rules_db.as_retriever()
result=retriever.get_relevant_documents("what is band?",
                                        top_k=3)
pretty_print_docs(result)

from langchain.llms import OpenAI, Cohere


from langchain.chains import RetrievalQA
#llm=OpenAI(temperature=0)
llm=Cohere(temperature=0)

Q_AChain=RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",# It takes a list of documents, inserts them all into a prompt
    retriever=retriever
)

query="What is MCC and MNC?"
docs=Q_AChain({"query":query})
docs["result"]

query="give me the vlaue of MCC and MNC from my data?"
docs=Q_AChain({"query":query})
docs["result"]

