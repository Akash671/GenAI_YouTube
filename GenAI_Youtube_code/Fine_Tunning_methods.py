# -*- coding: utf-8 -*-

"""
author : @akash
"""


"""Solution_HuggingFace_Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Pkmi8bP9tCP_mEGxaY8wAW9F4eVWs_S
"""

!pip -q install accelerate -U
!pip -q install transformers[torch]
!pip -q install datasets
!pip install --upgrade pyarrow
#Restart after installing

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from transformers import pipeline
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
from transformers import Trainer, TrainingArguments
from datasets import load_dataset, DatasetDict, ClassLabel, Dataset

"""## Import Emotions Data"""

!wget https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Final_Emotion_Data/five_emotions_data.csv
emotions_data=pd.read_csv("five_emotions_data.csv")
print(emotions_data.shape)
print(emotions_data.head())
print(emotions_data["label"].value_counts())

"""## Use distilbert model without finetunung"""

# Distil bert model
from transformers import pipeline
distilbert_model = pipeline(task="text-classification",
                            model="distilbert-base-uncased",
                            device="cuda",
                            )

sample_data=emotions_data.sample(10000, random_state=42)
sample_data["Text"]=sample_data["Text"].apply(lambda x: " ".join(x.split()[:100]))
sample_data["bert_predicted"] = sample_data["Text"].apply(lambda x: distilbert_model(x)[0]["label"])
sample_data["bert_predicted_num"]=sample_data["bert_predicted"].apply(lambda x: x[-1])
sample_data["bert_predicted_num"] = sample_data["bert_predicted_num"].astype(int)
sample_data.head()

"""### Accuracy of the model without fine-tuning"""

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(sample_data["label"], sample_data["bert_predicted_num"])
print(cm)
accuracy=cm.diagonal().sum()/cm.sum()
print(accuracy)

"""# Finetuning the model with our data

"""

from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
from transformers import Trainer, TrainingArguments
from datasets import load_dataset, DatasetDict, ClassLabel, Dataset
import pandas as pd
from sklearn.model_selection import train_test_split
import torch

Sample_data = Dataset.from_pandas(sample_data)
# Split the dataset into training and testing sets
train_test_split = Sample_data.train_test_split(test_size=0.2)  # 80% training, 20% testing
dataset = DatasetDict({
    'train': train_test_split['train'],
    'test': train_test_split['test']
})
dataset

"""### Load the tokenizer"""

# Load the tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Padding
tokenizer.pad_token = tokenizer.eos_token
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.add_special_tokens({'pad_token': '[PAD]'} )

def tokenize_function(examples):
    return tokenizer(examples["Text"], padding="max_length", truncation=True, max_length=100)
tokenized_datasets = dataset.map(tokenize_function, batched=True)

"""### Load and Train the model"""

model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',
                                                            num_labels=5,
                                                            pad_token_id=tokenizer.eos_token_id) # Adjust num_labels as needed

training_args = TrainingArguments(
    output_dir="./results_bert_custom",
    num_train_epochs=5,
    logging_dir="./logs_bert_custom",
    evaluation_strategy="epoch"

)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
)

# Start training
trainer.train()

# Define the directory where you want to save your model and tokenizer
model_dir = "./distilbert_finetuned"

# Save the model
model.save_pretrained(model_dir)

# Save the tokenizer
tokenizer.save_pretrained(model_dir)

#Save the model with
trainer.save_model('Distilbert_CustomModel_10K')

#!zip -r distilbert_finetuned_10k.zip ./distilbert_finetuned

def make_prediction(text):
  new_text=text
  inputs=tokenizer(new_text, return_tensors="pt")
  inputs = inputs.to(torch.device("cuda:0"))
  outputs=model(**inputs)
  predictions=outputs.logits.argmax(-1)
  predictions=predictions.detach().cpu().numpy()
  return(predictions)

sample_data["finetuned_predicted"]=sample_data["Text"].apply(lambda x: make_prediction(str(x))[0])

from sklearn.metrics import confusion_matrix
# Create the confusion matrix
cm1 = confusion_matrix(sample_data["label"], sample_data["finetuned_predicted"])
print(cm1)
accuracy1=cm1.diagonal().sum()/cm1.sum()
print(accuracy1)

"""### Loading a pre-built model and making prediction"""

#Code to donwloading the distilbert model
!gdown --id 12rYkcG7AHkZMDIlzJ4P5JkVCJwnXJvaU -O distilbert_finetuned_10k.zip
!unzip -o -j distilbert_finetuned_10k.zip -d distilbert_finetuned_V1

model_v1 = DistilBertForSequenceClassification.from_pretrained('/content/distilbert_finetuned_V1')
model_v1.to("cuda:0")

def make_prediction(text):
  new_complaint=text
  inputs=tokenizer(new_complaint, return_tensors="pt")
  inputs = inputs.to(torch.device("cuda:0"))
  outputs=model_v1(**inputs)
  predictions=outputs.logits.argmax(-1)
  predictions=predictions.detach().cpu().numpy()
  return(predictions)

sample_data_large=emotions_data.sample(n=40000, random_state=55)
sample_data_large["finetuned_predicted"]=sample_data_large["Text"].apply(lambda x: make_prediction(str(x))[0])

from sklearn.metrics import confusion_matrix
# Create the confusion matrix
cm1 = confusion_matrix(sample_data_large["label"], sample_data_large["finetuned_predicted"])
print(cm1)
accuracy1=cm1.diagonal().sum()/cm1.sum()
print(accuracy1)